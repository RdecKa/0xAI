import math

from sklearn.linear_model import LinearRegression

from learner import Learner
from model import Model


class LinearRegressionLearner(Learner):

    def __init__(self, splits, feature_names):
        super().__init__(None, feature_names)
        self.models = [LinearRegressionModel(feature_names, ind, splits[ind]) for ind, m in enumerate(splits)]

    @staticmethod
    def name():
        return "LinearRegressionLearner"

    @staticmethod
    def short_name():
        return "lrl"


class LinearRegressionModel(Model):

    class Submodel:

        def __init__(self, minimum, maximum, model):
            self.minimum = minimum
            self.maximum = maximum
            self.model = model

        def __str__(self):
            return "({} - {} - {})".format(self.minimum, self.maximum, self.model)

        def get_ID(self):
            return str(self.minimum) + "-" + str(self.maximum)

    def __init__(self, feature_names, ID, splits):
        super().__init__(None, feature_names)
        self.ID = "lrl_" + str(ID)
        self.submodels = [None] * (len(splits)+1)
        for split_index in range(len(splits)):
            minimum = 0 if split_index == 0 else splits[split_index-1] + 1
            maximum = splits[split_index]
            self.submodels[split_index] = \
                self.Submodel(minimum, maximum, LinearRegression(normalize=True, n_jobs=-1))
        self.submodels[-1] = \
            self.Submodel(maximum+1, math.inf, LinearRegression(normalize=True, n_jobs=-1))

    def __str__(self):
        s = ""
        for submodel in self.submodels:
            s += str(submodel)
        return s

    @staticmethod
    def name():
        return "lr"

    def feature_importances(self):
        s = []
        for submodel in self.submodels:
            s.append((self.ID + "." + submodel.get_ID(), submodel.model.coef_))
        return s

    def custom_output(self, model_index, outfolder):
        self.lr_to_go_code(model_index, outfolder)

    def lr_to_go_code(self, model_index, outfolder):
        coefficients = {}
        for submodel in self.submodels:
            coefficients[submodel.get_ID()] = submodel.model.coef_

        with open(outfolder + "linear" + str(model_index) + "code.go", "w") as code_file:
            def get_one_factor(feature_name, coefficient):
                return "({})*float64(s.{})".format(coefficient, feature_name)

            def get_one_submodel(key):
                z = [(fn, c) for (fn, c) in zip(self.feature_names, coefficients[key]) if c != 0]
                s = "\t\treturn {}".format(get_one_factor(*z[0]))
                for (n, c) in z[1:]:
                    s += " +\n\t\t\t{}".format(get_one_factor(n, c))
                s += "\n"
                return s
            code_file.write("// Package ab (Code generated by a Python script)\n")
            code_file.write("package ab\n\n")
            code_file.write("func getEstimatedValueLR(s Sample) float64 {\n")
            code_file.write("\tswitch {\n")

            for ind in range(len(self.submodels)):
                submodel = self.submodels[ind]
                if ind == len(self.submodels) - 1:
                    s = "\tdefault"
                else:
                    s = "\tcase s.num_stones <= " + str(submodel.maximum)
                s += ":\n"
                s += (get_one_submodel(submodel.get_ID()))
                code_file.write(s)
            code_file.write("\t}")  # end of switch
            code_file.write("\n}\n")

    def group_func(self, X, ind_X):
        """
        Tells to which group does a sample with index ind in the DataFrame X
        belong
        :param X: DataFrame
        :param ind: index of the investigated sample
        :return: group ID (from 0 to len(self.submodels)-1)
        """
        for ind in range(len(self.submodels)):  # Take advantage of submodels being ordered
            submodel = self.submodels[ind]
            if X["num_stones"].loc[ind_X] <= submodel.maximum:
                return ind
        return len(self.submodels) - 1

    def fit(self, X, y):
        data_in_subsets = X.groupby(lambda i: self.group_func(X, i))

        for ind in range(len(self.submodels)):
            if ind not in data_in_subsets.groups.keys():
                # No samples that would fall in this group
                self.submodels[ind] = None
                continue

            inp = X.loc[data_in_subsets.groups[ind]]
            out = y.loc[data_in_subsets.groups[ind]]
            self.submodels[ind].model.fit(inp, out)

        # Keep only models that are not None
        models = []
        for submodel in self.submodels:
            if submodel is not None:
                models.append(submodel)
        models[-1].maximum = math.inf  # Set the split of the last model to infinity

        self.submodels = models

    def predict(self, X):
        y = [0] * len(X)
        for (ind, X_ind) in enumerate(X.index):
            group_id = self.group_func(X, X_ind)
            y[ind] = self.submodels[group_id].model.predict([X.loc[X_ind]])

        return y

    def score(self, X, y):
        data_in_subsets = X.groupby(lambda i: self.group_func(X, i))

        s = {}
        for ind in range(len(self.submodels)):
            submodel = self.submodels[ind]

            if ind not in data_in_subsets.groups.keys():
                # No samples of this group in test data
                s[submodel.get_ID()] = None
                continue

            inp = X.loc[data_in_subsets.groups[ind]]
            out = y.loc[data_in_subsets.groups[ind]]
            s[submodel.get_ID()] = (submodel.model.score(inp, out), len(inp))
        for group in data_in_subsets.groups.keys():
            if group not in range(len(self.submodels)):
                inp = X.loc[data_in_subsets.groups[group]]
                s[str(group)] = ("Unknown", len(inp))
        return s
